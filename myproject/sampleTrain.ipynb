{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分類するクラス\n",
    "classes = ['1', '2', '3', '4', '5', '6', '7', \n",
    "          '8', '9', '10', '11']\n",
    "\n",
    "# それぞれ　egg_type_list の['Field', 'Undiscovered', 'Bug', 'Amorphous', 'Dragon', 'Fairy', 'Mineral', \n",
    "#                     'Flying', 'Grass', 'Human-Like', 'Monster', 'Water']　に対応\n",
    "\n",
    "\n",
    "nb_classes = len(classes)\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "# トレーニング用とバリデーション用の画像格納先\n",
    "train_data_dir = 'AllDataSet/train'\n",
    "validation_data_dir = 'AllDataSet/validation'\n",
    "\n",
    "# 今回はトレーニング用に664枚、バリデーション用に535枚の画像を用意した。\n",
    "nb_train_samples = 664\n",
    "nb_validation_samples = 534\n",
    "\n",
    "batch_size = 16\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_model_maker():\n",
    "    \"\"\" VGG16のモデルをFC層以外使用。FC層のみ作成して結合して用意する \"\"\"\n",
    "\n",
    "    # VGG16のロード。FC層は不要なので include_top=False\n",
    "    input_tensor = Input(shape=(img_width, img_height, 3))\n",
    "    vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "\n",
    "    # FC層の作成\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    top_model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "    # VGG16とFC層を結合してモデルを作成\n",
    "    model = Model(inputs=vgg16.input, outputs=top_model(vgg16.output))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator():\n",
    "    \"\"\" ディレクトリ内の画像を読み込んでトレーニングデータとバリデーションデータの作成 \"\"\"\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0 / 255,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        color_mode='rgb',\n",
    "        classes=classes,\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        color_mode='rgb',\n",
    "        classes=classes,\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    return (train_generator, validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = 'results'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.mkdir(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 663 images belonging to 11 classes.\n",
      "Found 534 images belonging to 11 classes.\n",
      "WARNING:tensorflow:From <ipython-input-7-a3e0891f9b22>:24: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/10\n",
      "41/41 [==============================] - ETA: 0s - loss: 2.5200 - accuracy: 0.1113"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/watanabeshingo/opt/anaconda3/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 133s 3s/step - loss: 2.5200 - accuracy: 0.1113 - val_loss: 2.3630 - val_accuracy: 0.1364\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 126s 3s/step - loss: 2.3370 - accuracy: 0.1561 - val_loss: 2.3701 - val_accuracy: 0.1534\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - 121s 3s/step - loss: 2.2912 - accuracy: 0.1901 - val_loss: 2.2652 - val_accuracy: 0.1913\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - 137s 3s/step - loss: 2.2140 - accuracy: 0.2102 - val_loss: 2.2537 - val_accuracy: 0.2405\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - 129s 3s/step - loss: 2.1233 - accuracy: 0.2365 - val_loss: 2.1238 - val_accuracy: 0.2670\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - 136s 3s/step - loss: 1.9922 - accuracy: 0.3014 - val_loss: 2.2453 - val_accuracy: 0.1875\n",
      "Epoch 7/10\n",
      "41/41 [==============================] - 135s 3s/step - loss: 1.9454 - accuracy: 0.3091 - val_loss: 2.0187 - val_accuracy: 0.2955\n",
      "Epoch 8/10\n",
      "41/41 [==============================] - 141s 3s/step - loss: 1.7572 - accuracy: 0.4049 - val_loss: 1.9011 - val_accuracy: 0.3068\n",
      "Epoch 9/10\n",
      "41/41 [==============================] - 158s 4s/step - loss: 1.6229 - accuracy: 0.4699 - val_loss: 1.9503 - val_accuracy: 0.2973\n",
      "Epoch 10/10\n",
      "41/41 [==============================] - 130s 3s/step - loss: 1.4438 - accuracy: 0.5085 - val_loss: 1.7913 - val_accuracy: 0.3902\n"
     ]
    }
   ],
   "source": [
    "# モデル作成\n",
    "vgg_model = vgg_model_maker()\n",
    "\n",
    "# 最後のconv層の直前までの層をfreeze\n",
    "for layer in vgg_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 多クラス分類を指定\n",
    "vgg_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=1e-3, momentum=0.9),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# 画像のジェネレータ生成\n",
    "train_generator, validation_generator = image_generator()\n",
    "\n",
    "# Fine-tuning\n",
    "# history = vgg_model.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=nb_train_samples,\n",
    "#     epochs=nb_epoch,\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=nb_validation_samples)\n",
    "\n",
    "history = vgg_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=int(663/batch_size),\n",
    "    epochs=nb_epoch,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=int(534/batch_size)\n",
    ")\n",
    "\n",
    "vgg_model.save_weights(os.path.join(result_dir, 'finetuning.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
