{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CJSJ-ML-8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Last-Vega/Klis_Workshop_MachineLearning/blob/master/CJSJ_ML_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUV_ZpMdF0qp"
      },
      "source": [
        "## PyTorchによるテキスト処理（再帰型ニューラルネットワークの利用）\n",
        "\n",
        "前回使用したデータに対して，再帰型ニューラルネットワークの一種であるLSTM (Long Short Term Memory)を利用する．\n",
        "\n",
        "**なお，再帰型ニューラルネットワークの学習はCPUでは時間がかかりすぎてしまうため，\n",
        "「ランタイム」メニュー→「ランタイムのタイプを変更」を選んでハードウェアアクセラレータを「GPU」に変更しておこう．**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzhbvle71nPn",
        "outputId": "9d29d6e1-1560-4e17-fd03-50f53279e4bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# torchtextというパッケージをアップデートしておく（古いバージョンの場合は後で出現する単語埋め込みが読み込めない）\n",
        "!pip install -U torchtext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/f9/224b3893ab11d83d47fde357a7dcc75f00ba219f34f3d15e06fe4cb62e05/torchtext-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (4.5MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5MB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.6.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.91 torchtext-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48WpVvuDJrfp",
        "outputId": "e89bed8c-21f5-4667-d9d1-59894176f911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# 分かち書きを行うためにjanomeというPythonパッケージをインストール\n",
        "!pip install janome"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.3MB/s \n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCwxJR4ZJGVF",
        "outputId": "0fad2521-1e68-4685-f6d4-d2ed3fdfd906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# ファイルのダウンロード\n",
        "!wget \"https://drive.google.com/uc?export=download&id=1c0tXuRt2GE8szurDI01P0YkbgfEuNi6o\" -O twitterJSA_data.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-23 05:43:51--  https://drive.google.com/uc?export=download&id=1c0tXuRt2GE8szurDI01P0YkbgfEuNi6o\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.142.100, 74.125.142.113, 74.125.142.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.142.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-58-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nlovaqil0ggj6dumdt2s09ikk1ruuacd/1603431825000/07803272131756145988/*/1c0tXuRt2GE8szurDI01P0YkbgfEuNi6o?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-10-23 05:43:52--  https://doc-10-58-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nlovaqil0ggj6dumdt2s09ikk1ruuacd/1603431825000/07803272131756145988/*/1c0tXuRt2GE8szurDI01P0YkbgfEuNi6o?e=download\n",
            "Resolving doc-10-58-docs.googleusercontent.com (doc-10-58-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-10-58-docs.googleusercontent.com (doc-10-58-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘twitterJSA_data.csv’\n",
            "\n",
            "twitterJSA_data.csv     [ <=>                ]   3.77M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-10-23 05:43:52 (207 MB/s) - ‘twitterJSA_data.csv’ saved [3958479]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhWASpyBm-6v"
      },
      "source": [
        "## TorchTextによるテキストデータの読み込み\n",
        "\n",
        "前回と同様のコードである．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfUa3i-bJUPZ",
        "outputId": "071f8da8-04c1-4814-a324-27f3ab06d5e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from torchtext import data\n",
        "from janome.tokenizer import Tokenizer # janomeからTokenizerクラスをimportする\n",
        "\n",
        "# janomeはPythonで書かれた形態素解析器であり単語の分かち書きなどが可能\n",
        "tokenizer = Tokenizer(wakati=True)\n",
        "\n",
        "def tokenize(text):\n",
        "    # 単語の分かち書きを行う\n",
        "    return list(tokenizer.tokenize(text))\n",
        "\n",
        "# Fieldオブジェクトは読み込んだデータの各項目（CSVの場合は各列）をどのように処理するかを決定する\n",
        "# Fieldの引数に何も指定しない場合には，テキストのような系列データと仮定し指定されたtokenizerで処理される\n",
        "# 今回用いるファイル中のidやlabelのように，系列データでなく，tokenizeする必要もないデータの場合は\n",
        "# sequential=False, use_vocab=Falseを指定する\n",
        "ID = data.Field(sequential=False, use_vocab=False)\n",
        "LABEL = data.Field(sequential=False, use_vocab=False)\n",
        "TEXT = data.Field(tokenize=tokenize) # tokenize引数に指定した関数でテキストを処理する\n",
        "\n",
        "# CSVファイルを読み込み\n",
        "dataset = data.TabularDataset(\n",
        "    path='./twitterJSA_data.csv', # 読み込みファイル\n",
        "    format='csv', # 読み込むファイルの形式\n",
        "    fields=[('id', ID), ('label', LABEL), ('text', TEXT)], # 各列ごとにFieldオブジェクトを設定\n",
        "    skip_header=True # 最初の行は各列の見出しなので読み込まない\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGGBUZVDOW3D",
        "outputId": "fabf1f7d-45fc-4025-a53e-c114a145e078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = 20000 # 先頭の20,000件を訓練データとして用いることにする\n",
        "batch_size = 32 # ミニバッチのサイズを設定\n",
        "\n",
        "# 訓練データとテストデータに分割\n",
        "train_dataset, test_dataset = dataset.split(split_ratio=train_size/len(dataset))\n",
        "\n",
        "# 訓練データを読み込むためのイテレータを準備\n",
        "train_iterator = data.BucketIterator(train_dataset, batch_size=batch_size, train=True)\n",
        "test_iterator = data.BucketIterator(test_dataset, batch_size=batch_size, train=False, sort=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsUi5qACklUN"
      },
      "source": [
        "# TEXTフィールド中の単語を収集して，単語の種類数や頻度などを計算し，これをVocabクラスのオブジェクトしてまとめて，\n",
        "# TEXTフィールドのデータ属性として保存する\n",
        "TEXT.build_vocab(train_dataset, min_freq=5) # `min_freq=5`: 頻度が5回未満の単語は無視する設定"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiGDxZDYsiCK"
      },
      "source": [
        "## テキスト分類の機械学習モデル\n",
        "\n",
        "前回の分類モデルでは，各単語をベクトルに変換し，1つのテキストをそのベクトル集合の平均値で表現していた．\n",
        "今回は，LSTMを用いて1つのテキストを表現し，文中の最後の単語に対応する隠れ層のベクトルを用いて，分類を行うことにする．\n",
        "\n",
        "LSTMは以下のように関数として表現することができる：\n",
        "\n",
        "$$\n",
        "{\\mathbf h}_t = {\\rm LSTM}({\\mathbf x}_t, {\\mathbf h}_{t-1})\n",
        "$$\n",
        "\n",
        "ただし，PyTorchにおいては，系列長 x バッチサイズ x 単語埋め込みの次元数，となっているテンソルが入力となる．より具体的には，テンソル`x`は以下のように表現される：\n",
        "\n",
        "```\n",
        "x = [\n",
        "  [\n",
        "    [0.1, 0.2, 0, 0.5],\n",
        "    [0, 0.3, 0.1, 0.1]\n",
        "  ],\n",
        "  [\n",
        "    [0.3, 0.1, 0, 0.1],\n",
        "    [0.3, 0.1, 0, 0.1]\n",
        "  ],\n",
        "  [\n",
        "    [0.2, 0.2, 0.2, 0],\n",
        "    [0.3, 0, 0.3, 0.3]\n",
        "  ]\n",
        "]\n",
        "```\n",
        "\n",
        "この例は，系列長3, バッチサイズ2, 埋め込みの次元4である時の例である．この表現は非常に見にくくなっているが，例えば，以下のような文が表現されている：\n",
        "\n",
        "```\n",
        "私 は 鳥\n",
        "海 は 青い\n",
        "```\n",
        "\n",
        "「私」の単語埋め込みは`[0.1, 0.2, 0, 0.5]`，「は」の単語埋め込みは`[0.3, 0.1, 0, 0.1]`である．テンソル中の単語埋め込みをそれを表現する単語で当てはめてみると以下のようになる（これはあくまで説明のための表現である）：\n",
        "\n",
        "```\n",
        "x = [\n",
        "  [\n",
        "    私,\n",
        "    海\n",
        "  ],\n",
        "  [\n",
        "    は,\n",
        "    は\n",
        "  ],\n",
        "  [\n",
        "    鳥,\n",
        "    青い\n",
        "  ]\n",
        "]\n",
        "```\n",
        "\n",
        "上記のようなテンソル`x`を入力するとPyTorchで実装されるLSTMは以下の値を返す：\n",
        "\n",
        "```\n",
        "output, (h_n, c_n) = LSTM(x)\n",
        "```\n",
        "\n",
        "ここで，`output`は系列長 x バッチサイズ x LSTMの隠れ層の出力の次元数，であるようなテンソルである．これを`h`とすると，以下のように表現することができる：\n",
        "\n",
        "```\n",
        "output = [\n",
        "  [\n",
        "    h^1_1,\n",
        "    h^2_1\n",
        "  ],\n",
        "  [\n",
        "    h^1_2,\n",
        "    h^2_2\n",
        "  ],\n",
        "  [\n",
        "    h^1_3,\n",
        "    h^2_3\n",
        "  ]\n",
        "]\n",
        "```\n",
        "\n",
        "ここで，`h^i_j`はミニバッチ中の`i`番目の入力に対する`j`番目の単語に対応する隠れ状態である．\n",
        "これはベクトルであり，例えば，`h^1_2 = [0.1, 0.5]`である．\n",
        "\n",
        "LSTMの出力のうち，`h_n`を今回のモデルでは利用しているが，\n",
        "これは1 x バッチサイズ x LSTMの隠れ層の出力の次元数，であるようなテンソルであり，下記のように表現することができる．\n",
        "\n",
        "```\n",
        "h_n = [\n",
        "  [\n",
        "    h^1_3,\n",
        "    h^2_3\n",
        "  ]\n",
        "]\n",
        "```\n",
        "\n",
        "つまり，`output`の一番最後の単語に対応する隠れ状態が`h_n`には含まれているのである．このテンソルのうち，最初の要素を取り出して，文の極性判定に用いる．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbYoXNDm1LpT",
        "outputId": "f5a1fd3d-db3d-4bf1-d8f0-10e78be93fcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from torchtext.vocab import FastText\n",
        "\n",
        "# FastTextの単語埋め込みを設定し，TEXT.vocabを更新\n",
        "# 単語埋め込みをダウンロードする必要があるため少し時間がかかる．\n",
        "TEXT.build_vocab(train_dataset, min_freq=5, vectors=FastText(language=\"ja\")) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/wiki.ja.vec: 1.37GB [00:37, 36.6MB/s]                            \n",
            "  0%|          | 0/580000 [00:00<?, ?it/s]Skipping token b'580000' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|█████████▉| 579348/580000 [01:01<00:00, 9587.43it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iijUuKLaspCY"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab):\n",
        "        super(TextClassifier, self).__init__()\n",
        "\n",
        "        # 単語埋め込み（FastText）を設定する\n",
        "        self.embedding = torch.nn.Embedding.from_pretrained(\n",
        "            embeddings=vocab.vectors, freeze=False)\n",
        "        \n",
        "        emb_size = vocab.vectors.shape[1]\n",
        "        self.lstm = nn.LSTM(emb_size, 100) # LSTMを用意する\n",
        "        self.fc = nn.Linear(100, 1) # 1 x 100 の行列（この場合はベクトル）を含む全結合層を設定\n",
        "\n",
        "    def forward(self, seq):\n",
        "        x = self.embedding(seq) # 各単語を単語埋め込みに変換する\n",
        "        output, (h_n, c_n) = self.lstm(x) # LSTMを適用する\n",
        "        h = h_n.view(-1, 100) # 各系列の最後の入力（単語）に対応する隠れ状態は，1 x バッチサイズ x 隠れ状態の次元数，となっているため，これを，バッチサイズ x 隠れ状態の次元数，と変換する．\n",
        "        y = self.fc(F.relu(h)) # ReLUをかけてから，全結合層に通す．\n",
        "        y = y.squeeze() # yは バッチサイズ x 1 という行列になっているため，バッチサイズと同じ次元数を持つベクトルに変換\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYLs-4it6brt"
      },
      "source": [
        "## テキスト分類モデルの学習\n",
        "\n",
        "訓練データを用いて先ほど定義した`TextClassifier`モデルを学習してみよう．学習の方法は以前と同じである．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cco70K69LRLS",
        "outputId": "7d172241-abf1-4d95-ef05-b87de748f8df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # CPUもしくはGPUのどちらを使うかを設定\n",
        "model = TextClassifier(TEXT.vocab) # ニューラルネットワークモデルのインスタンスを生成\n",
        "model = model.to(device) # CPUもしくはGPUのどちらを設定\n",
        "optimizer = optim.Adam(model.parameters()) # 基本的な学習方法はミニバッチ勾配降下法ではあるが，その中でもよく用いられるAdamと呼ばれる方法を用いることにする\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss() # 二値分類用の交差エントロピーを最小化することにする\n",
        "\n",
        "epoch_size = 20 # 勾配降下法はすべてのデータでパラメータを更新したら終わりではなく，全データでの更新（=1エポック）を複数回行う必要がある\n",
        "\n",
        "model.train() # モデルを学習モードに変更\n",
        "\n",
        "# `epoch_size`の数だけ以下を繰り返す\n",
        "for epoch in range(epoch_size):\n",
        "    losses = []\n",
        "    # イテレータはミニバッチ勾配降下法のために，`batch_size`で指定した数ごとにデータをわけて読み込んでくれる．\n",
        "    for batch_idx, batch in enumerate(train_iterator):\n",
        "        texts, labels = batch.text.to(device), batch.label.to(device)\n",
        "        optimizer.zero_grad() # 勾配の初期化\n",
        "        y = model(texts) # 現時点でのモデルの出力を得る\n",
        "        loss = criterion(y, labels.type(torch.float)) # 交差エントロピーの計算\n",
        "        loss.backward() # 交差エントロピーの勾配計算\n",
        "        optimizer.step() # パラメータ更新\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # 現在の交差エントロピーを出力\n",
        "    print('Epoch: {}\\tCross Entropy: {:.6f}'.format(epoch, sum(losses)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tCross Entropy: 419.375494\n",
            "Epoch: 1\tCross Entropy: 418.348388\n",
            "Epoch: 2\tCross Entropy: 374.574986\n",
            "Epoch: 3\tCross Entropy: 345.800831\n",
            "Epoch: 4\tCross Entropy: 335.364478\n",
            "Epoch: 5\tCross Entropy: 328.278812\n",
            "Epoch: 6\tCross Entropy: 322.935677\n",
            "Epoch: 7\tCross Entropy: 316.670601\n",
            "Epoch: 8\tCross Entropy: 320.220716\n",
            "Epoch: 9\tCross Entropy: 315.594265\n",
            "Epoch: 10\tCross Entropy: 308.648645\n",
            "Epoch: 11\tCross Entropy: 302.427961\n",
            "Epoch: 12\tCross Entropy: 297.587169\n",
            "Epoch: 13\tCross Entropy: 291.268057\n",
            "Epoch: 14\tCross Entropy: 285.316853\n",
            "Epoch: 15\tCross Entropy: 282.442409\n",
            "Epoch: 16\tCross Entropy: 276.323446\n",
            "Epoch: 17\tCross Entropy: 240.559083\n",
            "Epoch: 18\tCross Entropy: 195.557814\n",
            "Epoch: 19\tCross Entropy: 169.479286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PdfIHuy_pA5"
      },
      "source": [
        "## テキスト分類モデルの評価\n",
        "\n",
        "再び，テキスト分類モデルを評価してみよう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE1N1luwtSbA",
        "outputId": "874f3d9c-1d14-4218-c05e-6edf401792c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "correct = 0\n",
        "model.eval() # モデルを評価モードに変更\n",
        "for batch_idx, batch in enumerate(test_iterator):\n",
        "    texts, labels = batch.text.to(device), batch.label.to(device)\n",
        "    y = model(texts) # モデルの出力を得る\n",
        "    result = torch.sigmoid(y) # `TextClassifier`ではsigmoid関数を適用していなかったのでここで適用\n",
        "    prediction = result >= 0.5 # `result`ベクトルと同じ次元を持ち，`result`の中で0.5以上である次元がTrue，それ以外がFalseであるベクトルを`prediction`とする\n",
        "    target = labels == 1 # `labels`ベクトルと同じ次元を持ち，`labels`の中で1である次元がTrue，それ以外がFalseであるベクトルを`target`とする\n",
        "    correct_num = target.eq(prediction).sum().item() # `prediction`ベクトルと`target`ベクトルでTrue/Falseが一致したものの数を数える\n",
        "    correct += correct_num\n",
        "\n",
        "# test_iterator.datasetにはテストデータ全体が入っているので，これの長さはテストデータの事例数となる\n",
        "print(\"Accuracy: {:.3f}\".format(correct / len(test_iterator.dataset)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeCzpI65_zlm"
      },
      "source": [
        "## 今回のテキスト分類モデルのまとめ\n",
        "\n",
        "モデルの精度はどうであっただろうか．以前に試したモデルと比較してみると良いだろう．\n",
        "\n",
        "私も意外に思ったが，そこまで精度が出なかったのではないかと思われる．\n",
        "興味のある人は，エポック数を増やしたり，少しニューラルネットワークモデルを改良してみると良いだろう．"
      ]
    }
  ]
}